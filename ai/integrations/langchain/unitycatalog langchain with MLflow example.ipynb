{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an APP with Langgraph that uses UC functions as tools\n",
    "\n",
    "Prerequisite: Run this notebook on databricks platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7f8c9fc-edea-4144-8348-caba1afed572",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/unitycatalog/unitycatalog.git#subdirectory=ai/core\n",
      "  Cloning https://github.com/unitycatalog/unitycatalog.git to /tmp/pip-req-build-efagquet\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/unitycatalog/unitycatalog.git /tmp/pip-req-build-efagquet\n",
      "  Resolved https://github.com/unitycatalog/unitycatalog.git to commit 92532036aed87098eb0490886185d7bed5f4727b\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: unitycatalog-ai\n",
      "  Building wheel for unitycatalog-ai (pyproject.toml): started\n",
      "  Building wheel for unitycatalog-ai (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for unitycatalog-ai: filename=unitycatalog_ai-0.1.0-py3-none-any.whl size=31649 sha256=b5a64de0fc68d195d38c016931445c8ad983208e98bd7664cc7be1924cc7bf57\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-4tgwauy_/wheels/13/0d/20/8b3533dd8fc2b569f1cd13e7406cabbe055c648037f826517b\n",
      "Successfully built unitycatalog-ai\n",
      "Installing collected packages: unitycatalog-ai\n",
      "Successfully installed unitycatalog-ai-0.1.0\n",
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n",
      "Collecting git+https://github.com/unitycatalog/unitycatalog.git#subdirectory=ai/integrations/langchain\n",
      "  Cloning https://github.com/unitycatalog/unitycatalog.git to /tmp/pip-req-build-gcpv26mt\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/unitycatalog/unitycatalog.git /tmp/pip-req-build-gcpv26mt\n",
      "  Resolved https://github.com/unitycatalog/unitycatalog.git to commit 92532036aed87098eb0490886185d7bed5f4727b\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: unitycatalog-ai in /local_disk0/.ephemeral_nfs/envs/pythonEnv-dec52b04-7b0e-4c7a-a792-37744b0041b8/lib/python3.10/site-packages (from unitycatalog-langchain==0.1.0) (0.1.0)\n",
      "Collecting langchain-community>=0.2.0\n",
      "  Downloading langchain_community-0.3.5-py3-none-any.whl (2.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/2.4 MB 10.2 MB/s eta 0:00:00\n",
      "Collecting langchain>=0.2.0\n",
      "  Downloading langchain-0.3.7-py3-none-any.whl (1.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 18.0 MB/s eta 0:00:00\n",
      "Collecting langsmith<0.2.0,>=0.1.17\n",
      "  Downloading langsmith-0.1.140-py3-none-any.whl (304 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 304.8/304.8 kB 17.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /databricks/python3/lib/python3.10/site-packages (from langchain>=0.2.0->unitycatalog-langchain==0.1.0) (8.1.0)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.0\n",
      "  Downloading langchain_text_splitters-0.3.2-py3-none-any.whl (25 kB)\n",
      "Collecting PyYAML>=5.3\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 751.2/751.2 kB 22.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from langchain>=0.2.0->unitycatalog-langchain==0.1.0) (2.28.1)\n",
      "Collecting langchain-core<0.4.0,>=0.3.15\n",
      "  Downloading langchain_core-0.3.15-py3-none-any.whl (408 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 408.7/408.7 kB 11.2 MB/s eta 0:00:00\n",
      "Collecting SQLAlchemy<3,>=1.4\n",
      "  Downloading SQLAlchemy-2.0.36-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 30.2 MB/s eta 0:00:00\n",
      "Collecting aiohttp<4.0.0,>=3.8.3\n",
      "  Downloading aiohttp-3.10.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 24.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy<2,>=1 in /databricks/python3/lib/python3.10/site-packages (from langchain>=0.2.0->unitycatalog-langchain==0.1.0) (1.23.5)\n",
      "Collecting pydantic<3.0.0,>=2.7.4\n",
      "  Downloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 434.9/434.9 kB 44.3 MB/s eta 0:00:00\n",
      "Collecting async-timeout<5.0.0,>=4.0.0\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4\n",
      "  Downloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 45.4 MB/s eta 0:00:00\n",
      "Collecting dataclasses-json<0.7,>=0.5.7\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0\n",
      "  Downloading pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\n",
      "Collecting httpx-sse<0.5.0,>=0.4.0\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Requirement already satisfied: typing-extensions in /databricks/python3/lib/python3.10/site-packages (from unitycatalog-ai->unitycatalog-langchain==0.1.0) (4.4.0)\n",
      "Collecting aiohappyeyeballs>=2.3.0\n",
      "  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Collecting yarl<2.0,>=1.12.0\n",
      "  Downloading yarl-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 318.7/318.7 kB 41.7 MB/s eta 0:00:00\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 241.9/241.9 kB 42.1 MB/s eta 0:00:00\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.6/124.6 kB 28.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.2.0->unitycatalog-langchain==0.1.0) (22.1.0)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Downloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.5/49.5 kB 9.1 MB/s eta 0:00:00\n",
      "Collecting typing-inspect<1,>=0.4.0\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting typing-extensions\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /databricks/python3/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain>=0.2.0->unitycatalog-langchain==0.1.0) (23.2)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.5/54.5 kB 10.5 MB/s eta 0:00:00\n",
      "Collecting orjson<4.0.0,>=3.9.14\n",
      "  Downloading orjson-3.10.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 142.5/142.5 kB 19.6 MB/s eta 0:00:00\n",
      "Collecting httpx<1,>=0.23.0\n",
      "  Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.4/76.4 kB 9.6 MB/s eta 0:00:00\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.23.4\n",
      "  Downloading pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 43.1 MB/s eta 0:00:00\n",
      "Collecting python-dotenv>=0.21.0\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests<3,>=2->langchain>=0.2.0->unitycatalog-langchain==0.1.0) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests<3,>=2->langchain>=0.2.0->unitycatalog-langchain==0.1.0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests<3,>=2->langchain>=0.2.0->unitycatalog-langchain==0.1.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.10/site-packages (from requests<3,>=2->langchain>=0.2.0->unitycatalog-langchain==0.1.0) (1.26.14)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (599 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 599.5/599.5 kB 53.7 MB/s eta 0:00:00\n",
      "Collecting httpcore==1.*\n",
      "  Downloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.0/78.0 kB 18.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: anyio in /databricks/python3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain>=0.2.0->unitycatalog-langchain==0.1.0) (3.5.0)\n",
      "Requirement already satisfied: sniffio in /databricks/python3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain>=0.2.0->unitycatalog-langchain==0.1.0) (1.2.0)\n",
      "Collecting h11<0.15,>=0.13\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 12.1 MB/s eta 0:00:00\n",
      "Collecting jsonpointer>=1.9\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community>=0.2.0->unitycatalog-langchain==0.1.0) (0.4.3)\n",
      "Collecting propcache>=0.2.0\n",
      "  Downloading propcache-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 208.9/208.9 kB 28.3 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: unitycatalog-langchain\n",
      "  Building wheel for unitycatalog-langchain (pyproject.toml): started\n",
      "  Building wheel for unitycatalog-langchain (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for unitycatalog-langchain: filename=unitycatalog_langchain-0.1.0-py3-none-any.whl size=4600 sha256=977ff020a8852d10c7784dfdf585714618ca48d3479e0ae784647999ee9dcfdb\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-davw3x3b/wheels/f4/d8/8b/1b3c3c21dea29b826b9c670b3400e2ccdaa9efa9d529b8c599\n",
      "Successfully built unitycatalog-langchain\n",
      "Installing collected packages: typing-extensions, PyYAML, python-dotenv, propcache, orjson, marshmallow, jsonpointer, httpx-sse, h11, greenlet, frozenlist, async-timeout, annotated-types, aiohappyeyeballs, typing-inspect, SQLAlchemy, requests-toolbelt, pydantic-core, multidict, jsonpatch, httpcore, aiosignal, yarl, pydantic, httpx, dataclasses-json, pydantic-settings, langsmith, aiohttp, langchain-core, langchain-text-splitters, langchain, langchain-community, unitycatalog-langchain\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Not uninstalling typing-extensions at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-dec52b04-7b0e-4c7a-a792-37744b0041b8\n",
      "    Can't uninstall 'typing_extensions'. No files were found to uninstall.\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.6\n",
      "    Not uninstalling pydantic at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-dec52b04-7b0e-4c7a-a792-37744b0041b8\n",
      "    Can't uninstall 'pydantic'. No files were found to uninstall.\n",
      "Successfully installed PyYAML-6.0.2 SQLAlchemy-2.0.35 aiohappyeyeballs-2.4.3 aiohttp-3.10.10 aiosignal-1.3.1 annotated-types-0.7.0 async-timeout-4.0.3 dataclasses-json-0.6.7 frozenlist-1.5.0 greenlet-3.1.1 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 httpx-sse-0.4.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.7 langchain-community-0.3.5 langchain-core-0.3.15 langchain-text-splitters-0.3.2 langsmith-0.1.140 marshmallow-3.23.1 multidict-6.1.0 orjson-3.10.11 propcache-0.2.0 pydantic-2.9.2 pydantic-core-2.23.4 pydantic-settings-2.6.1 python-dotenv-1.0.1 requests-toolbelt-1.0.0 typing-extensions-4.12.2 typing-inspect-0.9.0 unitycatalog-langchain-0.1.0 yarl-1.17.1\n",
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n",
      "Collecting langchain_openai\n",
      "  Downloading langchain_openai-0.2.6-py3-none-any.whl (50 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.4/50.4 kB 2.3 MB/s eta 0:00:00\n",
      "Collecting langgraph\n",
      "  Downloading langgraph-0.2.45-py3-none-any.whl (119 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119.3/119.3 kB 6.2 MB/s eta 0:00:00\n",
      "Collecting openai<2.0.0,>=1.54.0\n",
      "  Downloading openai-1.54.1-py3-none-any.whl (389 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 389.3/389.3 kB 19.3 MB/s eta 0:00:00\n",
      "Collecting tiktoken<1,>=0.7\n",
      "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 61.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-dec52b04-7b0e-4c7a-a792-37744b0041b8/lib/python3.10/site-packages (from langchain_openai) (0.3.15)\n",
      "Collecting langgraph-checkpoint<3.0.0,>=2.0.0\n",
      "  Downloading langgraph_checkpoint-2.0.2-py3-none-any.whl (23 kB)\n",
      "Collecting langgraph-sdk<0.2.0,>=0.1.32\n",
      "  Downloading langgraph_sdk-0.1.35-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-dec52b04-7b0e-4c7a-a792-37744b0041b8/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_openai) (2.9.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-dec52b04-7b0e-4c7a-a792-37744b0041b8/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_openai) (0.1.140)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /databricks/python3/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_openai) (8.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-dec52b04-7b0e-4c7a-a792-37744b0041b8/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_openai) (4.12.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-dec52b04-7b0e-4c7a-a792-37744b0041b8/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_openai) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-dec52b04-7b0e-4c7a-a792-37744b0041b8/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_openai) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /databricks/python3/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_openai) (23.2)\n",
      "Collecting msgpack<2.0.0,>=1.1.0\n",
      "  Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 378.0/378.0 kB 43.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: httpx>=0.25.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-dec52b04-7b0e-4c7a-a792-37744b0041b8/lib/python3.10/site-packages (from langgraph-sdk<0.2.0,>=0.1.32->langgraph) (0.27.2)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-dec52b04-7b0e-4c7a-a792-37744b0041b8/lib/python3.10/site-packages (from langgraph-sdk<0.2.0,>=0.1.32->langgraph) (3.10.11)\n",
      "Requirement already satisfied: httpx-sse>=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-dec52b04-7b0e-4c7a-a792-37744b0041b8/lib/python3.10/site-packages (from langgraph-sdk<0.2.0,>=0.1.32->langgraph) (0.4.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /databricks/python3/lib/python3.10/site-packages (from openai<2.0.0,>=1.54.0->langchain_openai) (3.5.0)\n",
      "Collecting tqdm>4\n",
      "  Downloading tqdm-4.66.6-py3-none-any.whl (78 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.3/78.3 kB 18.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.54.0->langchain_openai) (1.7.0)\n",
      "Collecting jiter<1,>=0.4.0\n",
      "  Downloading jiter-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (327 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 327.5/327.5 kB 53.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: sniffio in /databricks/python3/lib/python3.10/site-packages (from openai<2.0.0,>=1.54.0->langchain_openai) (1.2.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in /databricks/python3/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2.28.1)\n",
      "Collecting regex>=2022.1.18\n",
      "  Downloading regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (782 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 782.7/782.7 kB 39.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: idna>=2.8 in /databricks/python3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.54.0->langchain_openai) (3.4)\n",
      "Requirement already satisfied: httpcore==1.* in /local_disk0/.ephemeral_nfs/envs/pythonEnv-dec52b04-7b0e-4c7a-a792-37744b0041b8/lib/python3.10/site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.32->langgraph) (1.0.6)\n",
      "Requirement already satisfied: certifi in /databricks/python3/lib/python3.10/site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.32->langgraph) (2022.12.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-dec52b04-7b0e-4c7a-a792-37744b0041b8/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.32->langgraph) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-dec52b04-7b0e-4c7a-a792-37744b0041b8/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain_openai) (3.0.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-dec52b04-7b0e-4c7a-a792-37744b0041b8/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_openai) (1.0.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-dec52b04-7b0e-4c7a-a792-37744b0041b8/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain_openai) (2.23.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-dec52b04-7b0e-4c7a-a792-37744b0041b8/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain_openai) (0.7.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (1.26.14)\n",
      "Installing collected packages: tqdm, regex, msgpack, jiter, tiktoken, openai, langgraph-sdk, langgraph-checkpoint, langchain_openai, langgraph\n",
      "Successfully installed jiter-0.7.0 langchain_openai-0.2.6 langgraph-0.2.45 langgraph-checkpoint-2.0.2 langgraph-sdk-0.1.35 msgpack-1.1.0 openai-1.54.1 regex-2024.9.11 tiktoken-0.8.0 tqdm-4.66.6\n",
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n",
      "Collecting git+https://github.com/mlflow/mlflow.git@master\n",
      "  Cloning https://github.com/mlflow/mlflow.git (to revision master) to /tmp/pip-req-build-ibm4uv89\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/mlflow/mlflow.git /tmp/pip-req-build-ibm4uv89\n",
      "  Resolved https://github.com/mlflow/mlflow.git to commit 41369e4296b62425444e61ebf42d850755fcceff\n",
      "  Running command git submodule update --init --recursive -q\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.17.3.dev0) (2.28.1)\n",
      "Collecting Flask<4\n",
      "  Downloading flask-3.0.3-py3-none-any.whl (101 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 101.7/101.7 kB 1.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.17.3.dev0) (3.1.2)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,<9,>=3.7.0 in /usr/lib/python3/dist-packages (from mlflow==2.17.3.dev0) (4.6.4)\n",
      "Requirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.17.3.dev0) (8.0.4)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-dec52b04-7b0e-4c7a-a792-37744b0041b8/lib/python3.10/site-packages (from mlflow==2.17.3.dev0) (2.0.35)\n",
      "Collecting graphene<4\n",
      "  Downloading graphene-3.4.1-py2.py3-none-any.whl (114 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.7/114.7 kB 6.9 MB/s eta 0:00:00\n",
      "Collecting gitpython<4,>=3.1.9\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.3/207.3 kB 18.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-dec52b04-7b0e-4c7a-a792-37744b0041b8/lib/python3.10/site-packages (from mlflow==2.17.3.dev0) (6.0.2)\n",
      "Collecting sqlparse<1,>=0.4.0\n",
      "  Downloading sqlparse-0.5.1-py3-none-any.whl (44 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 44.2/44.2 kB 9.7 MB/s eta 0:00:00\n",
      "Collecting opentelemetry-api<3,>=1.9.0\n",
      "  Downloading opentelemetry_api-1.28.0-py3-none-any.whl (64 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.3/64.3 kB 11.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.17.3.dev0) (1.1.1)\n",
      "Requirement already satisfied: pyarrow<19,>=4.0.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.17.3.dev0) (8.0.0)\n",
      "Collecting docker<8,>=4.0.0\n",
      "  Downloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147.8/147.8 kB 26.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: protobuf<6,>=3.12.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.17.3.dev0) (4.25.5)\n",
      "Requirement already satisfied: packaging<25 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.17.3.dev0) (23.2)\n",
      "Collecting gunicorn<24\n",
      "  Downloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.0/85.0 kB 20.1 MB/s eta 0:00:00\n",
      "Collecting markdown<4,>=3.3\n",
      "  Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106.3/106.3 kB 24.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: cachetools<6,>=5.0.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.17.3.dev0) (5.3.2)\n",
      "Collecting cloudpickle<4\n",
      "  Downloading cloudpickle-3.1.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: scipy<2 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.17.3.dev0) (1.10.0)\n",
      "Collecting opentelemetry-sdk<3,>=1.9.0\n",
      "  Downloading opentelemetry_sdk-1.28.0-py3-none-any.whl (118 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 118.7/118.7 kB 24.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.17.3.dev0) (3.7.0)\n",
      "Requirement already satisfied: pandas<3 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.17.3.dev0) (1.5.3)\n",
      "Requirement already satisfied: numpy<3 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.17.3.dev0) (1.23.5)\n",
      "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.17.3.dev0) (0.20.0)\n",
      "Collecting alembic!=1.10.0,<2\n",
      "  Downloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 233.5/233.5 kB 45.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-dec52b04-7b0e-4c7a-a792-37744b0041b8/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow==2.17.3.dev0) (4.12.2)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.3.6-py3-none-any.whl (78 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.6/78.6 kB 6.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.10/site-packages (from databricks-sdk<1,>=0.20.0->mlflow==2.17.3.dev0) (2.28.1)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /databricks/python3/lib/python3.10/site-packages (from docker<8,>=4.0.0->mlflow==2.17.3.dev0) (1.26.14)\n",
      "Collecting itsdangerous>=2.1.2\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Collecting Werkzeug>=3.0.0\n",
      "  Downloading werkzeug-3.1.2-py3-none-any.whl (224 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.4/224.4 kB 37.9 MB/s eta 0:00:00\n",
      "Collecting blinker>=1.6.2\n",
      "  Downloading blinker-1.8.2-py3-none-any.whl (9.5 kB)\n",
      "Collecting click<9,>=7.0\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97.9/97.9 kB 19.6 MB/s eta 0:00:00\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 12.1 MB/s eta 0:00:00\n",
      "Collecting graphql-core<3.3,>=3.1\n",
      "  Downloading graphql_core-3.2.5-py3-none-any.whl (203 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 203.2/203.2 kB 28.9 MB/s eta 0:00:00\n",
      "Collecting graphql-relay<3.3,>=3.1\n",
      "  Downloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /databricks/python3/lib/python3.10/site-packages (from graphene<4->mlflow==2.17.3.dev0) (2.8.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.10/site-packages (from Jinja2<4,>=2.11->mlflow==2.17.3.dev0) (2.1.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.17.3.dev0) (4.25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.17.3.dev0) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.17.3.dev0) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.17.3.dev0) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.17.3.dev0) (9.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.17.3.dev0) (1.0.5)\n",
      "Collecting deprecated>=1.2.6\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting importlib-metadata!=4.7.0,<9,>=3.7.0\n",
      "  Downloading importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\n",
      "Collecting zipp>=3.20\n",
      "  Downloading zipp-3.20.2-py3-none-any.whl (9.2 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.49b0\n",
      "  Downloading opentelemetry_semantic_conventions-0.49b0-py3-none-any.whl (159 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 159.2/159.2 kB 31.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.10/site-packages (from pandas<3->mlflow==2.17.3.dev0) (2022.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow==2.17.3.dev0) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow==2.17.3.dev0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow==2.17.3.dev0) (3.4)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /databricks/python3/lib/python3.10/site-packages (from scikit-learn<2->mlflow==2.17.3.dev0) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.10/site-packages (from scikit-learn<2->mlflow==2.17.3.dev0) (2.2.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-dec52b04-7b0e-4c7a-a792-37744b0041b8/lib/python3.10/site-packages (from sqlalchemy<3,>=1.4.0->mlflow==2.17.3.dev0) (3.1.1)\n",
      "Collecting wrapt<2,>=1.10\n",
      "  Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.3/80.3 kB 16.8 MB/s eta 0:00:00\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.10/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow==2.17.3.dev0) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.10/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow==2.17.3.dev0) (0.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow==2.17.3.dev0) (1.16.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /databricks/python3/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow==2.17.3.dev0) (0.5.1)\n",
      "Building wheels for collected packages: mlflow\n",
      "  Building wheel for mlflow (pyproject.toml): started\n",
      "  Building wheel for mlflow (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for mlflow: filename=mlflow-2.17.3.dev0-py3-none-any.whl size=5716701 sha256=7cb1b82c144a18cd6c3ce3e6870cdc8b1e82982b000822f977e62f58ef4463ae\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-gsgjp_yg/wheels/91/bd/b5/77a6d4396adf4c87367f3ed473a71a99791001fb1e4d92adf6\n",
      "Successfully built mlflow\n",
      "Installing collected packages: zipp, wrapt, Werkzeug, sqlparse, smmap, markdown, Mako, itsdangerous, gunicorn, graphql-core, cloudpickle, click, blinker, importlib-metadata, graphql-relay, gitdb, Flask, docker, deprecated, alembic, opentelemetry-api, graphene, gitpython, opentelemetry-semantic-conventions, opentelemetry-sdk, mlflow\n",
      "  Attempting uninstall: zipp\n",
      "    Found existing installation: zipp 1.0.0\n",
      "    Not uninstalling zipp at /usr/lib/python3/dist-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-dec52b04-7b0e-4c7a-a792-37744b0041b8\n",
      "    Can't uninstall 'zipp'. No files were found to uninstall.\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.0.4\n",
      "    Not uninstalling click at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-dec52b04-7b0e-4c7a-a792-37744b0041b8\n",
      "    Can't uninstall 'click'. No files were found to uninstall.\n",
      "  Attempting uninstall: blinker\n",
      "    Found existing installation: blinker 1.4\n",
      "    Not uninstalling blinker at /usr/lib/python3/dist-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-dec52b04-7b0e-4c7a-a792-37744b0041b8\n",
      "    Can't uninstall 'blinker'. No files were found to uninstall.\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 4.6.4\n",
      "    Not uninstalling importlib-metadata at /usr/lib/python3/dist-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-dec52b04-7b0e-4c7a-a792-37744b0041b8\n",
      "    Can't uninstall 'importlib-metadata'. No files were found to uninstall.\n",
      "Successfully installed Flask-3.0.3 Mako-1.3.6 Werkzeug-3.1.2 alembic-1.14.0 blinker-1.8.2 click-8.1.7 cloudpickle-3.1.0 deprecated-1.2.14 docker-7.1.0 gitdb-4.0.11 gitpython-3.1.43 graphene-3.4.1 graphql-core-3.2.5 graphql-relay-3.2.0 gunicorn-23.0.0 importlib-metadata-8.5.0 itsdangerous-2.2.0 markdown-3.7 mlflow-2.17.3.dev0 opentelemetry-api-1.28.0 opentelemetry-sdk-1.28.0 opentelemetry-semantic-conventions-0.49b0 smmap-5.0.1 sqlparse-0.5.1 wrapt-1.16.0 zipp-3.20.2\n",
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install unitycatalog-ai[databricks] unitycatalog-langchai langchain_openai langgraph mlflow\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18072ae8-83e5-4880-9b89-a92ee9f67641",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Create a DatabricksFunctionClient and set as the default one\n",
    "Use Serverless here as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fae8cd2a-6f52-4da2-8dbb-4d95d8284e28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from unitycatalog.ai.core.client import set_uc_function_client\n",
    "from unitycatalog.ai.core.databricks import DatabricksFunctionClient\n",
    "\n",
    "client = DatabricksFunctionClient()\n",
    "\n",
    "# sets the default uc function client\n",
    "set_uc_function_client(client)\n",
    "\n",
    "# replace with your own catalog and schema\n",
    "CATALOG = \"ml\"\n",
    "SCHEMA = \"serena_test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5a6f448-21a5-4fe4-82e3-f952df401834",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Create UC functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9509aeb8-d4c7-41b9-af8f-473ffdd5379c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunctionExecutionResult(error=None, format='SCALAR', value='2\\n', truncated=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def execute_python_code(code: str) -> str:\n",
    "  \"\"\"\n",
    "  Executes the given python code and returns its stdout.\n",
    "  Remember the code should print the final result to stdout.\n",
    "\n",
    "  Args:\n",
    "    code: Python code to execute. Remember to print the final result to stdout.\n",
    "  \"\"\"\n",
    "  import sys\n",
    "  from io import StringIO\n",
    "  \n",
    "  stdout = StringIO()\n",
    "  sys.stdout = stdout\n",
    "  exec(code)\n",
    "  return stdout.getvalue()\n",
    "\n",
    "function_info = client.create_python_function(func=execute_python_code, catalog=CATALOG, schema=SCHEMA, replace=True)\n",
    "python_execution_function_name = function_info.full_name\n",
    "\n",
    "# test execution\n",
    "client.execute_function(python_execution_function_name, {\"code\": \"print(1+1)\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a0c6a93-192f-46c8-bba3-ec81748207ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was developed by Databricks and is designed to help data scientists and machine learning engineers manage the complexities of building, deploying, and maintaining ML models.\\n\\nMLflow provides a set of tools and APIs that enable users to:\\n\\n1. **Track experiments**: Record and manage the parameters, metrics, and artifacts of ML experiments, making it easier to reproduce and compare results.\\n2. **Manage models**: Store, version, and deploy ML models, including support for multiple frameworks such as TensorFlow, PyTorch, and Scikit-learn.\\n3. **Deploy models**: Deploy ML models to various environments, including cloud, on-premises, and edge devices.\\n4. **Monitor and manage models**: Track the performance of deployed models, detect data drift, and retrain models as needed.\\n\\nMLflow consists of four main components:\\n\\n1. **MLflow Tracking**: Records and manages the parameters, metrics, and artifacts of ML experiments.\\n2. **MLflow Projects**: Provides a standardized way to package and deploy ML models.\\n3. **MLflow Models**: Manages the deployment and serving of ML models.\\n4. **MLflow Registry**: Provides a centralized repository for storing and managing ML models.\\n\\nMLflow supports a wide range of ML frameworks and libraries, including:\\n\\n* TensorFlow\\n* PyTorch\\n* Scikit-learn\\n* XGBoost\\n* LightGBM\\n* Keras\\n\\nMLflow is widely used in the industry and has been adopted by many organizations, including those in finance, healthcare, and technology.\\n\\nSome of the benefits of using MLflow include:\\n\\n* **Improved collaboration**: MLflow enables data scientists and engineers to collaborate more effectively by providing a standardized way to manage ML experiments and models.\\n* **Increased productivity**: MLflow automates many tasks, such as tracking experiments and deploying models, freeing up users to focus on more strategic tasks.\\n* **Better model management**: MLflow provides a centralized repository for managing ML models, making it easier to track and manage model performance over time.\\n\\nOverall, MLflow is a powerful tool for managing the ML lifecycle, and its adoption is growing rapidly in the industry.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_ai_function_name = f\"{CATALOG}.{SCHEMA}.ask_ai\"\n",
    "sql_body = f\"\"\"CREATE OR REPLACE FUNCTION {ask_ai_function_name}(question STRING COMMENT 'question to ask')\n",
    "RETURNS STRING\n",
    "COMMENT 'answer the question using Meta-Llama-3.1-70B-Instruct model'\n",
    "RETURN SELECT ai_gen(question)\n",
    "\"\"\"\n",
    "client.create_function(sql_function_body=sql_body)\n",
    "result = client.execute_function(ask_ai_function_name, {\"question\": \"What is MLflow?\"})\n",
    "result.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ddd7433-13f4-4cd4-b48f-d9a2681489a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunctionExecutionResult(error=None, format='SCALAR', value='MLflow: End-to-End Machine Learning Lifecycle Management', truncated=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarization_function_name = f\"{CATALOG}.{SCHEMA}.summarize\"\n",
    "sql_body = f\"\"\"CREATE OR REPLACE FUNCTION {summarization_function_name}(text STRING COMMENT 'content to parse', max_words INT COMMENT 'max number of words in the response, must be non-negative integer, if set to 0 then no limit')\n",
    "RETURNS STRING\n",
    "COMMENT 'summarize the content and limit response to max_words'\n",
    "RETURN SELECT ai_summarize(text, max_words)\n",
    "\"\"\"\n",
    "client.create_function(sql_function_body=sql_body)\n",
    "# test execution\n",
    "client.execute_function(summarization_function_name, {\"text\": result.value, \"max_words\": 20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05af21a7-1c09-4197-b4cb-5c80602183e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunctionExecutionResult(error=None, format='SCALAR', value='Hola', truncated=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_function_name = f\"{CATALOG}.{SCHEMA}.translate\"\n",
    "sql_body = f\"\"\"CREATE OR REPLACE FUNCTION {translate_function_name}(content STRING COMMENT 'content to translate', language STRING COMMENT 'target language')\n",
    "RETURNS STRING\n",
    "COMMENT 'translate the content to target language, currently only english <-> spanish translation is supported'\n",
    "RETURN SELECT ai_translate(content, language)\n",
    "\"\"\"\n",
    "client.create_function(sql_function_body=sql_body)\n",
    "# test execution\n",
    "client.execute_function(translate_function_name, {\"content\": \"Hello\", \"language\": \"es\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e61bb13-e40d-4d22-a0aa-f58367adba24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Create tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a383e331-7cd5-43a0-9728-c3ece9f2670c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "python_execution_function_name = f\"{CATALOG}.{SCHEMA}.execute_python_code\"\n",
    "ask_ai_function_name = f\"{CATALOG}.{SCHEMA}.ask_ai\"\n",
    "summarization_function_name = f\"{CATALOG}.{SCHEMA}.summarize\"\n",
    "translate_function_name = f\"{CATALOG}.{SCHEMA}.translate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "952ed06f-89c6-4495-81d6-ca1fe2a24c9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[UnityCatalogTool(name='ml__serena_test__execute_python_code', description='Executes the given python code and returns its stdout. Remember the code should print the final result to stdout.', args_schema=<class 'unitycatalog.ai.core.utils.function_processing_utils.ml__serena_test__execute_python_code__params'>, func=<function UCFunctionToolkit.uc_function_to_langchain_tool.<locals>.func at 0x7f566fe069e0>, uc_function_name='ml.serena_test.execute_python_code', client_config={'warehouse_id': None, 'profile': None}),\n",
       " UnityCatalogTool(name='ml__serena_test__ask_ai', description='answer the question using Meta-Llama-3.1-70B-Instruct model', args_schema=<class 'unitycatalog.ai.core.utils.function_processing_utils.ml__serena_test__ask_ai__params'>, func=<function UCFunctionToolkit.uc_function_to_langchain_tool.<locals>.func at 0x7f566fe07520>, uc_function_name='ml.serena_test.ask_ai', client_config={'warehouse_id': None, 'profile': None}),\n",
       " UnityCatalogTool(name='ml__serena_test__summarize', description='summarize the content and limit response to max_words', args_schema=<class 'unitycatalog.ai.core.utils.function_processing_utils.ml__serena_test__summarize__params'>, func=<function UCFunctionToolkit.uc_function_to_langchain_tool.<locals>.func at 0x7f566fe07a30>, uc_function_name='ml.serena_test.summarize', client_config={'warehouse_id': None, 'profile': None}),\n",
       " UnityCatalogTool(name='ml__serena_test__translate', description='translate the content to target language, currently only english <-> spanish translation is supported', args_schema=<class 'unitycatalog.ai.core.utils.function_processing_utils.ml__serena_test__translate__params'>, func=<function UCFunctionToolkit.uc_function_to_langchain_tool.<locals>.func at 0x7f566fd2c0d0>, uc_function_name='ml.serena_test.translate', client_config={'warehouse_id': None, 'profile': None})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unitycatalog.ai.langchain.toolkit import UCFunctionToolkit\n",
    "\n",
    "toolkit = UCFunctionToolkit(\n",
    "    function_names=[\n",
    "        python_execution_function_name,\n",
    "        ask_ai_function_name,\n",
    "        summarization_function_name,\n",
    "        translate_function_name,\n",
    "    ]\n",
    ")\n",
    "tools = toolkit.tools\n",
    "tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aafb2a37-27d6-42b9-8220-b71b5ab062ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Use the tools in Langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3d6a36b-42c4-480f-a446-752a11ca2bd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<REPLACE WITH YOUR OWN OPENAI_API_KEY>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f765a706-501d-4f02-abd2-fa96987b869c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# enable mlflow autologging so tracing is enabled\n",
    "# traces are super useful for debugging\n",
    "mlflow.langchain.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb3d0cb8-aafc-4023-a423-dca9119ff314",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langgraph.graph import END, START, MessagesState, StateGraph\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "tool_node = ToolNode(tools)\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\").bind_tools(tools)\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(state: MessagesState) -> Literal[\"tools\", END]:\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # If the LLM makes a tool call, then we route to the \"tools\" node\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    # Otherwise, we stop (reply to the user)\n",
    "    return END\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    messages = state['messages']\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "# This means that this node is the first one called\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    # First, we define the start node. We use `agent`.\n",
    "    # This means these are the edges taken after the `agent` node is called.\n",
    "    \"agent\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that after `tools` is called, `agent` node is called next.\n",
    "workflow.add_edge(\"tools\", 'agent')\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e74fdf7b-bf4c-4ea1-8909-372fa292700e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MLflow es una plataforma de código abierto para gestionar el ciclo de vida completo del aprendizaje automático. Fue desarrollado por Databricks y está diseñado para ayudar a los científicos de datos y a los ingenieros a manejar las complejidades de la creación, implementación y mantenimiento de modelos de aprendizaje automático.\\n\\nMLflow proporciona herramientas y API que permiten a los científicos de datos:\\n\\n1. **Seguimiento de experimentos**: Realizar un seguimiento y gestionar experimentos, incluida la optimización de hiperparámetros.\\n2. **Gestión de modelos**: Almacenar, versionar e implementar modelos.\\n3. **Implementación de modelos**: Implementar modelos en varios entornos, incluidos la nube y locales.\\n4. **Monitoreo y análisis**: Monitorear y analizar el rendimiento de los modelos.\\n\\nSus principales componentes son:\\n\\n1. **Seguimiento de MLflow**: Para rastrear experimentos.\\n2. **Proyectos de MLflow**: Empaquetar código en proyectos reutilizables.\\n3. **Modelos de MLflow**: Registro de modelos.\\n4. **Servicio de MLflow**: Implementar modelos en diferentes entornos.\\n\\nLos beneficios incluyen mejora en la colaboración, aumento de la reproducibilidad, implementación más rápida y mejor gestión de modelos. MLflow es una herramienta poderosa para gestionar el ciclo de vida del aprendizaje automático.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/databricks.mlflow.trace": "\"tr-b9ddcdb3bc42480294709255dcea80c7\"",
      "text/plain": [
       "Trace(request_id=tr-b9ddcdb3bc42480294709255dcea80c7)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_state = app.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What is MLflow? Keep the response concise and rely in Spanish. Try using as much tools as possible\",\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    ")\n",
    "response = final_state[\"messages\"][-1].content\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cc92803-9677-48e5-b7ed-04dd715d58a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is the translated explanation in English:\\n\\nMLflow is an open-source platform for managing the end-to-end machine learning lifecycle. It was developed by Databricks and is designed to help data scientists and engineers manage the complexities of creating, deploying, and maintaining machine learning models.\\n\\nMLflow provides tools and APIs that allow data scientists to:\\n\\n1. **Experiment tracking**: Track and manage experiments, including hyperparameter optimization.\\n2. **Model management**: Store, version, and deploy models.\\n3. **Model deployment**: Deploy models in various environments, including cloud and on-premises.\\n4. **Monitoring and analysis**: Monitor and analyze model performance.\\n\\nIts main components are:\\n\\n1. **MLflow tracking**: To track experiments.\\n2. **MLflow projects**: Package code into reusable projects.\\n3. **MLflow models**: Register models.\\n4. **MLflow model serving**: Deploy models in different environments.\\n\\nThe benefits include improved collaboration, increased reproducibility, faster deployment, and better model management. MLflow is a powerful tool for managing the machine learning lifecycle.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/databricks.mlflow.trace": "\"tr-efebb1ba511e49ee90f2085e4b54a9e6\"",
      "text/plain": [
       "Trace(request_id=tr-efebb1ba511e49ee90f2085e4b54a9e6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_state = app.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Remember to always try using tools. Can you convert following explantion to English? {response}\",\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    ")\n",
    "final_state[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "141e202a-a8b4-4e20-aea5-19144ce93162",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The result of \\\\(2^{10}\\\\) is 1024.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/databricks.mlflow.trace": "\"tr-52b8a7439b2a4344ae00868cf6af1e8a\"",
      "text/plain": [
       "Trace(request_id=tr-52b8a7439b2a4344ae00868cf6af1e8a)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_state = app.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is 2**10?\"}]},\n",
    ")\n",
    "final_state[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3390ecd8-726f-4219-ab85-cc4d61385324",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Log the model using MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model to a separate file `app.py` using magic command `%%writefile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60108a27-c47b-4d22-9ad0-c6b2d9a4a26b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "from mlflow.models import set_model\n",
    "from unitycatalog.ai.core.client import set_uc_function_client\n",
    "from unitycatalog.ai.core.databricks import DatabricksFunctionClient\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, StateGraph, MessagesState\n",
    "from typing import Annotated, Literal, TypedDict\n",
    "from unitycatalog.ai.langchain.toolkit import UCFunctionToolkit\n",
    "\n",
    "client = DatabricksFunctionClient()\n",
    "\n",
    "# sets the default uc function client\n",
    "set_uc_function_client(client)\n",
    "\n",
    "# replace with your own catalog and schema\n",
    "CATALOG = \"ml\"\n",
    "SCHEMA = \"serena_test\"\n",
    "\n",
    "python_execution_function_name = f\"{CATALOG}.{SCHEMA}.execute_python_code\"\n",
    "ask_ai_function_name = f\"{CATALOG}.{SCHEMA}.ask_ai\"\n",
    "summarization_function_name = f\"{CATALOG}.{SCHEMA}.summarize\"\n",
    "translate_function_name = f\"{CATALOG}.{SCHEMA}.translate\"\n",
    "toolkit = UCFunctionToolkit(\n",
    "    function_names=[\n",
    "        python_execution_function_name,\n",
    "        ask_ai_function_name,\n",
    "        summarization_function_name,\n",
    "        translate_function_name,\n",
    "    ]\n",
    ")\n",
    "tools = toolkit.tools\n",
    "\n",
    "tool_node = ToolNode(tools)\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\").bind_tools(tools)\n",
    "\n",
    "def should_continue(state: MessagesState) -> Literal[\"tools\", END]:\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    messages = state['messages']\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_conditional_edges(\"agent\", should_continue)\n",
    "workflow.add_edge(\"tools\", 'agent')\n",
    "\n",
    "app = workflow.compile()\n",
    "set_model(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "100564c4-4a2e-4d1e-9919-0932613b3a53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e1fb3ea48f4cecbf02defcbe97c627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/06 10:59:49 INFO mlflow.models.model: Found the following environment variables used during model inference: [OPENAI_API_KEY]. Please check if you need to set them when deploying the model. To disable this message, set environment variable `MLFLOW_RECORD_ENV_VARS_IN_MODEL_LOGGING` to `false`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a236f3cd7df74e33ab506ae0b1c24ebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'ml.serena_test.app_with_tools' already exists. Creating a new version of this model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a8a5b155f0f4386a5b16c33c2db87c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '1' of model 'ml.serena_test.app_with_tools'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run rogue-midge-190 at: https://e2-dogfood.staging.cloud.databricks.com/ml/experiments/3916415516979169/runs/a00aabdcb6464fb09de01016a571cdfc\n",
      "🧪 View experiment at: https://e2-dogfood.staging.cloud.databricks.com/ml/experiments/3916415516979169\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "# temp workaround as the current output format is not supported by model signature\n",
    "input_example = {\"messages\": [{\"role\": \"user\", \"content\": \"What is 2**10?\"}]}\n",
    "signature = infer_signature(input_example, \"1024\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "  model_info = mlflow.langchain.log_model(\n",
    "    # Pass the path to the saved model file\n",
    "    \"app.py\",\n",
    "    \"model\",\n",
    "    input_example={\"messages\": [{\"role\": \"user\", \"content\": \"What is 3**10?\"}]},\n",
    "    signature=signature,\n",
    "    pip_requirements=[\n",
    "      \"mlflow\",\n",
    "      \"git+https://github.com/mlflow/mlflow.git\",\n",
    "      \"git+https://github.com/unitycatalog/unitycatalog.git#subdirectory=ai/core\",\n",
    "      \"git+https://github.com/unitycatalog/unitycatalog.git#subdirectory=ai/integrations/langchain\",\n",
    "      \"langchain_openai\",\n",
    "      \"langgraph\"\n",
    "    ],\n",
    "    registered_model_name=\"ml.serena_test.app_with_tools\", # Replace with your own model name\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62a96a45-254f-41da-8f47-a0ba83ccadf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Validate the model locally prior to serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "934afbd8-8c2c-40a6-ba47-c864487c57cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b1049b3436348d5b9e21dce2b89099d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'messages': [{'content': 'What is 3**10?',\n",
       "    'additional_kwargs': {},\n",
       "    'response_metadata': {},\n",
       "    'type': 'human',\n",
       "    'name': None,\n",
       "    'id': '82772f75-1123-4e1b-99d2-302199a5ef7f',\n",
       "    'example': False},\n",
       "   {'content': '',\n",
       "    'additional_kwargs': {'tool_calls': [{'id': 'call_UlCzoqHx05lOM4YfQsktqYv2',\n",
       "       'function': {'arguments': '{\"code\":\"print(3**10)\"}',\n",
       "        'name': 'ml__serena_test__execute_python_code'},\n",
       "       'type': 'function'}],\n",
       "     'refusal': None},\n",
       "    'response_metadata': {'token_usage': {'completion_tokens': 26,\n",
       "      'prompt_tokens': 286,\n",
       "      'total_tokens': 312,\n",
       "      'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "       'audio_tokens': 0,\n",
       "       'reasoning_tokens': 0,\n",
       "       'rejected_prediction_tokens': 0},\n",
       "      'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       "     'model_name': 'gpt-4o-mini-2024-07-18',\n",
       "     'system_fingerprint': 'fp_0ba0d124f1',\n",
       "     'finish_reason': 'tool_calls',\n",
       "     'logprobs': None},\n",
       "    'type': 'ai',\n",
       "    'name': None,\n",
       "    'id': 'run-4fafb419-26a3-40e1-9532-1ab6683335ab-0',\n",
       "    'example': False,\n",
       "    'tool_calls': [{'name': 'ml__serena_test__execute_python_code',\n",
       "      'args': {'code': 'print(3**10)'},\n",
       "      'id': 'call_UlCzoqHx05lOM4YfQsktqYv2',\n",
       "      'type': 'tool_call'}],\n",
       "    'invalid_tool_calls': [],\n",
       "    'usage_metadata': {'input_tokens': 286,\n",
       "     'output_tokens': 26,\n",
       "     'total_tokens': 312,\n",
       "     'input_token_details': {'audio': 0, 'cache_read': 0},\n",
       "     'output_token_details': {'audio': 0, 'reasoning': 0}}},\n",
       "   {'content': '{\"format\": \"SCALAR\", \"value\": \"59049\\\\n\"}',\n",
       "    'additional_kwargs': {},\n",
       "    'response_metadata': {},\n",
       "    'type': 'tool',\n",
       "    'name': 'ml__serena_test__execute_python_code',\n",
       "    'id': '3f9e0517-f486-4e8a-bcbf-bc5cb3737c55',\n",
       "    'tool_call_id': 'call_UlCzoqHx05lOM4YfQsktqYv2',\n",
       "    'artifact': None,\n",
       "    'status': 'success'},\n",
       "   {'content': 'The result of \\\\( 3^{10} \\\\) is 59049.',\n",
       "    'additional_kwargs': {'refusal': None},\n",
       "    'response_metadata': {'token_usage': {'completion_tokens': 17,\n",
       "      'prompt_tokens': 342,\n",
       "      'total_tokens': 359,\n",
       "      'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "       'audio_tokens': 0,\n",
       "       'reasoning_tokens': 0,\n",
       "       'rejected_prediction_tokens': 0},\n",
       "      'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       "     'model_name': 'gpt-4o-mini-2024-07-18',\n",
       "     'system_fingerprint': 'fp_0ba0d124f1',\n",
       "     'finish_reason': 'stop',\n",
       "     'logprobs': None},\n",
       "    'type': 'ai',\n",
       "    'name': None,\n",
       "    'id': 'run-13617be9-c59b-457c-acff-93b3a8f77327-0',\n",
       "    'example': False,\n",
       "    'tool_calls': [],\n",
       "    'invalid_tool_calls': [],\n",
       "    'usage_metadata': {'input_tokens': 342,\n",
       "     'output_tokens': 17,\n",
       "     'total_tokens': 359,\n",
       "     'input_token_details': {'audio': 0, 'cache_read': 0},\n",
       "     'output_token_details': {'audio': 0, 'reasoning': 0}}}]}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/databricks.mlflow.trace": "\"tr-b760a4d97e7d40eda469e96f0b34476e\"",
      "text/plain": [
       "Trace(request_id=tr-b760a4d97e7d40eda469e96f0b34476e)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mlflow.models import convert_input_example_to_serving_input, validate_serving_input\n",
    "\n",
    "serving_input = convert_input_example_to_serving_input({\"messages\": [{\"role\": \"user\", \"content\": \"What is 3**10?\"}]})\n",
    "validate_serving_input(model_info.model_uri, serving_input=serving_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c03c8948-2d31-4024-811a-de8fc593d9eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Deploy the model to a serving endpoint for production usage\n",
    "\n",
    "Follow [this guidance](https://docs.databricks.com/en/machine-learning/model-serving/create-manage-serving-endpoints.html#create-an-endpoint) to create a serving endpoint using the registered model.\n",
    "\n",
    "**Remember** to set the following environment variables when creating the serving endpoint:\n",
    "- DATABRICKS_HOST\n",
    "- DATABRICKS_TOKEN\n",
    "- OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10d17a13-95be-49a8-8f2f-556eeadb6cfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3916415516979323,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Langchain toolkit example",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
